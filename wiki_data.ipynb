{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "  if (x.ndim == 1):\n",
    "    x = x[None,:]    # ベクトル形状なら行列形状に変換\n",
    "  # テンソル（x：行列）、軸（axis=1： 列の横方向に計算）\n",
    "  return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "# 入力（x）と出力（y）の例\n",
    "x = np.array([[1,0,0], [0,1,0], [0,0,1]]) \n",
    "y = softmax(x)\n",
    "print(y)  # 以下のように出力される\n",
    "# [[0.57611688 0.21194156 0.21194156]  # → 猫： 全部足すと「1.0」になる\n",
    "#  [0.21194156 0.57611688 0.21194156]  # → 虎： Σ＝1.0\n",
    "#  [0.21194156 0.21194156 0.57611688]] # → ライオン： Σ＝1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mecab-python3 in c:\\users\\jikan\\anaconda3\\lib\\site-packages (1.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import MeCab\n",
    "import glob\n",
    "import os\n",
    "import traceback\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Sentence(object):\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.surfaces = []\n",
    "        self.features = []\n",
    "\n",
    "        if self.root:\n",
    "            node = root\n",
    "            while node:\n",
    "                self.surfaces.append(node.surface)\n",
    "                self.features.append(node.feature)\n",
    "                node = node.next\n",
    "\n",
    "    def all_words(self):\n",
    "        for surface, feature in zip(self.surfaces, self.features):\n",
    "            yield surface, feature\n",
    "\n",
    "    def word_count(self):\n",
    "        return len(self.surfaces)\n",
    "\n",
    "    def to_wakati(self):\n",
    "        return ' '.join([w for w in self.surfaces if w])\n",
    "\n",
    "class WakatiCorpus(object):\n",
    "    def __init__(self):\n",
    "        self.wakati_list = []\n",
    "\n",
    "    def run(self):\n",
    "        tagger = MeCab.Tagger('-Ochasen')\n",
    "        tagger.parseToNode('') # to prevent GC\n",
    "        \n",
    "        for text in wiki_sentences():\n",
    "            #print(text)\n",
    "            encoded_text = text\n",
    "            node = tagger.parseToNode(encoded_text)\n",
    "            sentence = Sentence(node)\n",
    "            wakati = sentence.to_wakati()\n",
    "            if wakati:\n",
    "                self.wakati_list.append(wakati + '\\n')\n",
    "\n",
    "    def load(self, filename):\n",
    "        with open(filename, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            self.wakati_list = [x.strip() for x in lines]\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(self.wakati_list)\n",
    "\n",
    "    def loaded(self):\n",
    "        return len(self.wakati_list) > 0\n",
    "\n",
    "\n",
    "class DocumentIterator(object):\n",
    "\n",
    "    WIKI_ROOT_DIR = './archive_wiki/jawiki_test'\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._i = 0\n",
    "        self.document_path = os.path.join(DocumentIterator.WIKI_ROOT_DIR, '*')\n",
    "        self._files = glob.glob(self.document_path)\n",
    "        self._docs = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "            if len(self._docs) > 0:\n",
    "                doc = self._docs.pop()\n",
    "                #print(self._docs[:5])\n",
    "                return doc\n",
    "            if self._i < len(self._files):\n",
    "                i = self._i\n",
    "                self._i += 1\n",
    "                self._docs = self._get_doc_list(self._files[i])\n",
    "                doc = self._docs.pop()\n",
    "                return doc\n",
    "            #print(self._docs[:5])\n",
    "            if len(self._docs) == 0:\n",
    "                raise StopIteration()\n",
    "\n",
    "\n",
    "    def _get_doc_list(self, filename):\n",
    "        try:\n",
    "            with open(filename,encoding='utf-8') as f:\n",
    "                xml = f.read()\n",
    "                #print(xml[:5])\n",
    "                soup = BeautifulSoup(xml, 'html.parser')\n",
    "                #print(soup)\n",
    "                docs = soup.find_all('doc')\n",
    "                #print(docs[:5])\n",
    "                #[print(doc.string[:5]) for doc in reversed(docs) if doc.string]\n",
    "                return [doc.string for doc in reversed(docs) if doc.string]\n",
    "        except:\n",
    "            print('Failed to read', filename, traceback.format_exc())\n",
    "            return []\n",
    "\n",
    "\n",
    "class SentenceIterator(object):\n",
    "    def __init__(self, document):\n",
    "        self._i = 0\n",
    "        self.sentences = self._break(document)\n",
    "\n",
    "    def _break(self, sentences):\n",
    "        lines = sentences.split('\\n')\n",
    "        return [line for line in lines if len(line) > 0]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._i < len(self.sentences):\n",
    "            i = self._i\n",
    "            self._i += 1\n",
    "            return self.sentences[i]\n",
    "        else:\n",
    "            raise StopIteration()\n",
    "\n",
    "def wiki_sentences():\n",
    "    #print(\"YesYes\")\n",
    "    diter = DocumentIterator()\n",
    "    #print(diter)\n",
    "    for doc in diter:\n",
    "        #print(doc[:5])\n",
    "        siter = SentenceIterator(doc)\n",
    "        for text in siter:\n",
    "            yield text\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wakati = WakatiCorpus()\n",
    "    wakati.run()\n",
    "    wakati.save('archive_wiki/wakati/wakati_corpus.txt')\n",
    "    #分かち書き文をファイル'archive_wiki/wakati/wakati_corpus.txt'に出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab[:5] ['意味', 'する', '記号', 'ラテン語', '合']\n",
      "len(vocab) 120099\n"
     ]
    }
   ],
   "source": [
    "import collections, os, pickle\n",
    "import nltk, MeCab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tokenize(sentences):\n",
    "    \"\"\"文章を分かち書きするとともに、ボキャブラリも返す。\n",
    "\n",
    "    :param sentences(str): 複数の文章を含む文字列。日本語想定。\n",
    "    :return(list):\n",
    "      tokens(list): 分かち書きした単語をlistとしてまとめたもの。\n",
    "      vocab(list): ボキャブラリ。ユニークな単語一覧。\n",
    "    \"\"\"\n",
    "\n",
    "    # 「。」、「！」、「？」で終わる一連の文字列を文として認識し分割する。\n",
    "    #動詞、名詞、形容詞、形容動詞以外を分かち書き、語彙から削除、しかしこの語彙は使わない\n",
    "    jp_sent_tokenizer = nltk.RegexpTokenizer(u'[^　「」！？。]*[！？。]')\n",
    "    tagger = MeCab.Tagger('-Owakati -d mecab-ipadic-neologd')\n",
    "\n",
    "    sents = jp_sent_tokenizer.tokenize(sentences)\n",
    "    tokens = []\n",
    "    vocab = []\n",
    "    for sent in sents:\n",
    "        node = tagger.parseToNode(sent)\n",
    "        while node:\n",
    "            features = node.feature.split(\",\")\n",
    "            base_word = features[6]\n",
    "            if base_word == \"*\" or base_word == \" \" or base_word == \"\\n\":\n",
    "                node = node.next\n",
    "                continue\n",
    "            if features[0] != \"名詞\"  and features[0] != \"動詞\" and features[0] != \"形容詞\" and features[0] != \"形容動詞\":\n",
    "                node = node.next\n",
    "                continue\n",
    "            tokens.append(base_word)\n",
    "            if base_word not in vocab:\n",
    "                vocab.append(base_word)\n",
    "            node = node.next\n",
    "    return tokens, vocab\n",
    "\n",
    "# 文章からボキャブラリと分かち書きを用意。\n",
    "filename = \"./archive_wiki/wakati/wakati_corpus.txt\"\n",
    "with open(filename, \"rb\") as fh:\n",
    "    sentences = \"\"\n",
    "    for line in fh.readlines():\n",
    "        try:\n",
    "            sentences += line.decode(\"utf-8\") + \" \"\n",
    "        except:\n",
    "            continue\n",
    "wakati_sentences, vocab = tokenize(sentences)\n",
    "print(\"vocab[:5]\", vocab[:5])\n",
    "print(\"len(vocab)\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "voc=Counter(wakati_sentences).most_common(20000)\n",
    "#語彙データから出現頻度順20000語を抽出\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['する', 'れる', 'いる', '年', 'なる']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc)\n",
    "new_voc=[voca[0] for voca in voc]\n",
    "new_voc[:5]\n",
    "#most_commonは出現回数も同時に出力するため単語のみを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-91c2a0f78364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'new_row' is not defined"
     ]
    }
   ],
   "source": [
    "new_row[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['哀', '悲悲悲'], ['愛', '好親安尊好親好'], ['相いれない', '不嫌嫌'], ['哀感', '悲悲寂悲'], ['愛敬', '親親好好親']]\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "# Excelファイルのロード(読み取り専用)\n",
    "excel_path='archive_wiki/D18/D18-2018.7.24.xlsx'\n",
    "workbook = load_workbook(filename=excel_path, read_only=True)\n",
    "\n",
    "# Excelのシート名一覧を表示\n",
    "#print(workbook.sheetnames)\n",
    "sheet=workbook[workbook.sheetnames[1]]\n",
    "rows=[]\n",
    "for row in sheet[f'A2:C{sheet.max_row}']:\n",
    "    values = []\n",
    "    for cell in row:\n",
    "        values.append(cell.value)\n",
    "    rows.append(values)\n",
    "# ロードしたExcelファイルを閉じる\n",
    "new_row=[[row[0],row[2]] for row in rows]\n",
    "\n",
    "sheet=workbook[workbook.sheetnames[2]]\n",
    "i=0\n",
    "for row in sheet[f'C2:C{sheet.max_row}']:\n",
    "    for cell in row:    \n",
    "        if cell.value is not None :new_row[i][1]+=cell.value\n",
    "    i=i+1\n",
    "sheet=workbook[workbook.sheetnames[3]]\n",
    "i=0\n",
    "for row in sheet[f'C2:C{sheet.max_row}']:\n",
    "    for cell in row:    \n",
    "        if cell.value is not None :new_row[i][1]+=cell.value\n",
    "    i=i+1\n",
    "\n",
    "print(new_row[:5])\n",
    "workbook.close()\n",
    "#感性語辞典のエクセルの感性語と,その感情データ3人分を合体させて保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['哀', '悲悲悲'],\n",
       " ['愛', '好親安尊好親好'],\n",
       " ['相いれない', '不嫌嫌'],\n",
       " ['哀感', '悲悲寂悲'],\n",
       " ['愛敬', '親親好好親']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wakati=MeCab.Tagger(\"-Owakati\")\n",
    "row_use=[]\n",
    "for d in new_row:\n",
    "    if d[0] == None or len(wakati.parse(d[0]).split())>1:\n",
    "        continue\n",
    "    else :row_use.append(d)\n",
    "print(len(row_use))\n",
    "row_use[:5]\n",
    "#分かち書きをすると2語以上になる単語を削除。同時にエクセルファイルを読み込むときに発生した空のデータを削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21192\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "for d in row_use:\n",
    "    if d[0] in new_voc:\n",
    "        continue\n",
    "    else:\n",
    "        new_voc.append(d[0])\n",
    "print(len(new_voc))\n",
    "#語彙のリストに含まれない感性語辞書の単語を語彙のリストに追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "する\n"
     ]
    }
   ],
   "source": [
    "# ボキャブラリと分かち書き文章から、データセットを作成。\n",
    "word_to_id = dict((c,i) for i,c in enumerate(new_voc))\n",
    "id_to_word = dict((i,c) for i,c in enumerate(new_voc))\n",
    "print(word_to_id[\"する\"])\n",
    "print(id_to_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8914226\n"
     ]
    }
   ],
   "source": [
    "new_wakati_sentence=[]\n",
    "for word in wakati_sentences:\n",
    "    if word in new_voc:\n",
    "        new_wakati_sentence.append(word)\n",
    "print(len(new_wakati_sentence))\n",
    "#分かち書きを行った文の単語に対して、語彙に含まれない物を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140, 0, 974, 2042, 3851]\n",
      "意味 する 記号 ラテン語 合\n"
     ]
    }
   ],
   "source": [
    "# 分かち書き文章を単語IDで表現\n",
    "wakati_ids = []\n",
    "for word in new_wakati_sentence:\n",
    "    wakati_ids.append(word_to_id[word])\n",
    "\n",
    "print(wakati_ids[:5])\n",
    "print(id_to_word[140], id_to_word[0], id_to_word[974], id_to_word[2042], id_to_word[3851])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に用いる分かち書き文章はnew_wakati_sentence、用いる語彙はnew_voc、感情値はrow_useに保存されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'wakati_ids' (list)\n",
      "Stored 'id_to_word' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store wakati_ids\n",
    "%store id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/oreilly-japan/deep-learning-from-scratch-2/blob/master/ch03/train.py\n",
    "#ここから CBOW 準備\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')  # 親ディレクトリのファイルをインポートするための設定\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from simple_cbow import SimpleCBOW\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot, cos_similarity\n",
    "\n",
    "window_size = 3\n",
    "hidden_size = 10 # 密ベクトルのサイズ\n",
    "batch_size = 100 # 一度に処理するサンプル数\n",
    "max_epoch = 1000 # 重み更新回数（学習回数）\n",
    "\n",
    "# データセットの準備\n",
    "vocab_size = len(new_voc)\n",
    "\n",
    "# ウィンドウサイズで指定された文脈と、その文脈下における対象語を収集\n",
    "contexts, target = create_contexts_target(wakati_ids, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "print(target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの準備\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習。必要に応じて学習済みファイルから読み込み。\n",
    "model_filename = \"word2vec_momotaro_model.pkl\"\n",
    "trainer_filename = \"word2vec_momotaro_trainer.pkl\"\n",
    "answer = \"n\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"use pretrained file ({})? [y/n]\".format(model_filename))\n",
    "    answer = input()\n",
    "if answer == \"n\":\n",
    "    trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "    trainer.plot()\n",
    "    pickle.dump(model, open(model_filename, 'wb'))\n",
    "    pickle.dump(trainer, open(trainer_filename, 'wb'))\n",
    "elif answer == \"y\":\n",
    "    model = pickle.load(open(model_filename, 'rb'))\n",
    "    trainer = pickle.load(open(trainer_filename, 'rb'))\n",
    "    trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習した密ベクトルを利用しやすいように整形\n",
    "embeddings = model.word_vecs\n",
    "word_to_vec = dict()\n",
    "for index, word in enumerate(vocab):\n",
    "    vec = embeddings[index]\n",
    "    word_to_vec[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(embeddings, word, n):\n",
    "    \"\"\"密ベクトル集合から、指定された単語に類似した単語を検索して出力する。\n",
    "\n",
    "    :param embeddings: 単語の密ベクトルを辞書型で保存したもの。{word:vector}\n",
    "    :param word: 検索対象語。str型。\n",
    "    :param n: 類似度が最も高いn件まで出力。\n",
    "    \"\"\"\n",
    "    distances = dict()\n",
    "    for w in embeddings.keys():\n",
    "        distances[w] = cos_similarity(embeddings[w],embeddings[word])\n",
    "    d_sorted = collections.OrderedDict(sorted(distances.items(),key = lambda x:x[1] ,reverse = True))\n",
    "    s_words = list(d_sorted.keys())\n",
    "    print(\"closest({})\".format(word))\n",
    "    print(s_words[:n])\n",
    "\n",
    "    # 参考用に3件まで類似度を出力\n",
    "    print(cos_similarity(embeddings[word], embeddings[s_words[0]]), word, s_words[0])\n",
    "    print(cos_similarity(embeddings[word], embeddings[s_words[1]]), word, s_words[1])\n",
    "    print(cos_similarity(embeddings[word], embeddings[s_words[2]]), word, s_words[2])\n",
    "    print(\"----\")\n",
    "\n",
    "closest(word_to_vec,'桃太郎',10)\n",
    "closest(word_to_vec,'おじいさん',10)\n",
    "closest(word_to_vec,'猿',10)\n",
    "closest(word_to_vec,'鬼',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "#OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.\n",
    "#上記エラーに対応するための環境変数設定。なお、エラー文全文によるとこの方法は非推奨の模様。\n",
    "#nokmlをインストールすることで解決できるケースが多いようだけど、試した限りでは途中で関連ライブラリのインストールが止まるため、今回は環境変数設定で対応することに。\n",
    "#参考：https://stackoverflow.com/questions/53648730/omp-error-15-initializing-libiomp5-dylib-but-found-libiomp5-dylib-already-in\n",
    "\n",
    "# フォント設定\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as font_manager\n",
    "font_path = '/Library/Fonts/Arial Unicode.ttf'\n",
    "font_prop = font_manager.FontProperties(fname = font_path)\n",
    "matplotlib.rcParams['font.family'] = font_prop.get_name()\n",
    "\n",
    "# TSNE設定\n",
    "from sklearn.manifold import TSNE\n",
    "tagger = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "labels = []\n",
    "tokens = []\n",
    "for w in word_to_vec.keys():\n",
    "    # 全ての単語を描画したい場合、ここでの品詞判定をせずに、全てlabels.appendすると良い。\n",
    "    temp = tagger.parse(w).split()[1]\n",
    "    pos = temp.split(',')[0]\n",
    "    if pos == '名詞':\n",
    "        labels.append(w)\n",
    "        tokens.append(word_to_vec[w])\n",
    "\n",
    "# 各種パラメータは適宜ドキュメント参照。ここでは固定したいのでシード値も設定。\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "new_values = tsne_model.fit_transform(tokens)\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i], y[i])\n",
    "    plt.annotate(labels[i],\n",
    "                 xy=(x[i], y[i]),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
